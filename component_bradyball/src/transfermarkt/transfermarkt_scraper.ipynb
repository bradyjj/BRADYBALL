{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef40106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_player_basic_data(headers):\n",
    "    # URL for the national competitions page\n",
    "    country_competitions_url = \"https://www.transfermarkt.us/wettbewerbe/national/wettbewerbe/189/saison_id/2023\"\n",
    "    response = requests.get(country_competitions_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all competitions links\n",
    "    competitions = []\n",
    "    competition_table = soup.find('table', class_='items')\n",
    "    for row in competition_table.find_all('tr'):\n",
    "        header = row.find('td', class_='extrarow')\n",
    "        if header and 'Tier' in header.text:\n",
    "            links = row.find_next_sibling('tr').find_all('a', href=True, title=True)\n",
    "            for link in links:\n",
    "                if 'wettbewerb' in link['href']:\n",
    "                    competitions.append(f\"https://www.transfermarkt.us{link['href']}\")\n",
    "\n",
    "    # Loop through each competition and scrape team data\n",
    "    all_player_data = []\n",
    "    for competition_url in tqdm(competitions, desc=\"Competitions\"):\n",
    "        response = requests.get(competition_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        teams = soup.find_all('a', href=True, class_='vereinprofil_tooltip')\n",
    "\n",
    "        for team in teams:\n",
    "            team_name = team.text.strip()\n",
    "            team_url = f\"https://www.transfermarkt.us{team['href']}kader/verein/{team['href'].split('/')[4]}/saison_id/2023/plus/1\"\n",
    "            team_response = requests.get(team_url, headers=headers)\n",
    "            team_soup = BeautifulSoup(team_response.content, \"html.parser\")\n",
    "            players = team_soup.find_all('tr', class_=['odd', 'even'])\n",
    "\n",
    "            for player in players:\n",
    "                name_tag = player.select_one('a[href^=\"/\"]')\n",
    "                if name_tag:\n",
    "                    all_player_data.append({\n",
    "                        'Team': team_name,\n",
    "                        'Name': name_tag.text.strip(),\n",
    "                        'URL': \"https://www.transfermarkt.us\" + name_tag['href'],\n",
    "                        'Position': player.find('td', class_='posrela').find_next('td').text.strip() if player.find('td', class_='posrela') else 'Position Not Found'\n",
    "                    })\n",
    "\n",
    "    return all_player_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe7831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_detailed_player_data(player_url, headers):\n",
    "    response = requests.get(player_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    player_id = player_url.split('/')[-1]\n",
    "\n",
    "    try:\n",
    "        # Use css selectors to find class and then get text, remove whitespace, remove null\n",
    "        player_name = soup.select_one('h1[class=\"data-header__headline-wrapper\"]').text.split('\\n')[-1].strip()\n",
    "    except AttributeError:\n",
    "        player_name = None\n",
    "\n",
    "    try:\n",
    "        # Use css selectors to find class and then get text, remove #, remove null\n",
    "        player_number = soup.select_one('span[class=\"data-header__shirt-number\"]').text.replace('#', '').strip()\n",
    "    except AttributeError:\n",
    "        player_number = None\n",
    "\n",
    "    try:\n",
    "        player_contract_expiry = re.search(r\"Contract expires: .*__content\\\">(.*?)</span>\", str(soup)).group(1)\n",
    "    except AttributeError:\n",
    "        player_contract_expiry = None\n",
    "\n",
    "    try:\n",
    "        player_foot = re.search(r\"Foot:</span>\\s*<span class=\\\"info-table__content info-table__content--bold\\\">(.*?)</span>\", str(soup)).group(1)\n",
    "    except AttributeError:\n",
    "        player_foot = None\n",
    "\n",
    "    try:\n",
    "        player_agent = re.search(r\"Player agent:</span>\\s*<span[^>]*>\\s*<a[^>]*>([^<]+)</a>\", str(soup)).group(1)\n",
    "    except AttributeError:\n",
    "        player_agent = None\n",
    "\n",
    "    try:\n",
    "        player_outfitter = re.search(r\"Outfitter:</span>\\s*<span class=\\\"info-table__content info-table__content--bold\\\">\\s*(.*?)\\s*</span>\", str(soup)).group(1)\n",
    "    except AttributeError:\n",
    "        player_outfitter = None\n",
    "\n",
    "    try:\n",
    "        player_citizenship = re.search(r\"Citizenship:</span>[\\s\\S]*?alt=\\\"([^\\\"]+)\\\"\", str(soup)).group(1)\n",
    "    except AttributeError:\n",
    "        player_citizenship = None\n",
    "\n",
    "    try:\n",
    "        player_contract_start = re.search(r\"Joined:</span>\\s*<span[^>]*>\\s*([^<]+)</span>\", str(soup)).group(1)\n",
    "    except AttributeError:\n",
    "        player_contract_start = None\n",
    "\n",
    "    try:\n",
    "        # Find the span that directly contains birthplace\n",
    "        birthplace_span = soup.find('span', itemprop=\"birthPlace\")\n",
    "        if birthplace_span:\n",
    "            city = birthplace_span.text.strip()\n",
    "            country_img = birthplace_span.find_previous('img', class_=\"flaggenrahmen\")\n",
    "            if country_img and country_img.has_attr('title'):\n",
    "                country = country_img['title'].strip()\n",
    "                player_birthplace = f\"{city}, {country}\"\n",
    "            else:\n",
    "                # If no country, just leave as city\n",
    "                player_birthplace = city\n",
    "        else:\n",
    "            player_birthplace = None\n",
    "    except AttributeError:\n",
    "        player_birthplace = None\n",
    "\n",
    "    # Organize data into a dictionary\n",
    "    player_data = {\n",
    "        'Name': player_name,\n",
    "        'Number': player_number,\n",
    "        'Contract Expiry': player_contract_expiry,\n",
    "        'Foot': player_foot,\n",
    "        'Agent': player_agent,\n",
    "        'Outfitter': player_outfitter,\n",
    "        'Citizenship': player_citizenship,\n",
    "        'Contract Start Date': player_contract_start,\n",
    "        'Birthplace': player_birthplace\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    player_df = pd.DataFrame([player_data])\n",
    "    return player_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96956662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Competitions: 100%|███████████████████████████████| 6/6 [00:20<00:00,  3.34s/it]\n",
      "Fetching Detailed Player Data: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Fetch basic data\n",
    "players_basic_data = fetch_player_basic_data(headers)\n",
    "\n",
    "# Fetch detailed data for each player\n",
    "final_data = []\n",
    "for player in tqdm(players_basic_data, desc=\"Fetching Detailed Player Data\"):\n",
    "    detailed_data = fetch_detailed_player_data(player['URL'], headers)\n",
    "    player.update(detailed_data)  # Merge dictionaries\n",
    "    final_data.append(player)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(final_data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58866fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3766ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
